{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Building a Neural Network from scratch\n",
        "\n",
        "Since we know the mathematical theory behind the working of a Neural Network, our goal is now to try and implement it (one can only learn by actually applying the learnt concepts).\n",
        "\n",
        "We first begin with the necessary imports."
      ],
      "metadata": {
        "id": "3juH1RzaRGDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axxLfmHGBly0"
      },
      "outputs": [],
      "source": [
        "import numpy as np #will be useful for constructing arrays\n",
        "from keras.datasets import mnist #the dataset we are trying to make our network learn\n",
        "import matplotlib.pyplot as plt #for plotting\n",
        "import math as m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the dataset\n",
        "\n",
        "It is always good practice to know the dataset inside out while building the network. It helps in deciding hyperparameters for the network (like how many neurons is optimal per layer and how many layers there need to be).\n",
        "\n",
        "For this we plot the images and use print commands to understand the shape and structure of all the arrays we are working with."
      ],
      "metadata": {
        "id": "NNbxsXkNqi8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() #importing the mnist dataset\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "eduNCzOdYAke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b21212b-1052-4548-c28e-e2b71dae913c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0].squeeze(), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "R0V8haD9qBMo",
        "outputId": "c913aa8f-ab8b-42e4-ea15-480c363695a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building the Neural Network\n",
        "\n",
        "Let us start defining the structure of the model. We also define functions for us to apply sigmoidal and softmax activations at each layer."
      ],
      "metadata": {
        "id": "WIctpA5dsw4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dimensions of the network: the inputs are 784 pixels corresponding to the image of the handwritten digit\n",
        "num_rows = 28\n",
        "num_inputs = 28*28\n",
        "\n",
        "#three hidden layers with 64, 40 and 16 neurons each\n",
        "num_h1 = 64\n",
        "num_h2 = 40\n",
        "num_h3 = 16\n",
        "\n",
        "#10 outputs corresponding to one digit each\n",
        "num_outputs = 10\n",
        "\n",
        "\n",
        "#initialize weights and biases as random values\n",
        "w_in1 = np.random.randn(num_h1, num_inputs)\n",
        "w_12 = np.random.randn(num_h2, num_h1)\n",
        "w_23 = np.random.randn(num_h3, num_h2)\n",
        "w_3ou = np.random.randn(num_outputs, num_h3)\n",
        "b_1 = np.random.randn(num_h1,)\n",
        "b_2 = np.random.randn(num_h2,)\n",
        "b_3 = np.random.randn(num_h3,)\n",
        "b_out = np.random.randn(num_outputs,)\n",
        "\n",
        "#defining activation functions used in this network along with their derivatives\n",
        "def sigmoid(x):\n",
        "  return 1/(1+ np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "  return np.divide(np.exp(x), np.sum(np.exp(x)))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x)*(1 - sigmoid(x))\n",
        "\n",
        "def softmax_derivative(x):\n",
        "  return softmax(x)*(1 - softmax(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "GHLZkZaxqNbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training the Neural Network\n",
        "\n",
        "We start training the network by applying forward passes and calculating the cost (we have chosen mean square error loss here). Then we apply gradient descent (partial derivatives are calculated from scratch using the chain rule) and find the optimal values of weights and biases."
      ],
      "metadata": {
        "id": "3AxL-7y-ONmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l = 0.01 #learning rate of the network\n",
        "\n",
        "num_grad_desc_steps = 10 #number of times we are going to perform a forward and a backward pass\n",
        "\n",
        "while True:\n",
        "\n",
        "  cost = 0 #cost is zero because it needs to be calculated for every forward pass independently\n",
        "  batch_size = 3 #number of images that we are training the network over\n",
        "\n",
        "  #converting the 28x28 image into a single input vector of size 784\n",
        "  for a in range(batch_size):\n",
        "    inp = []\n",
        "    for i in range(num_rows):\n",
        "      for j in range(num_rows):\n",
        "        inp.append(x_train[a][i][j]/255)\n",
        "\n",
        "    #forward pass\n",
        "    h_1 = np.matmul(w_in1, inp) + b_1\n",
        "    h_1 = sigmoid(h_1) #applying a sigmoidal activation\n",
        "\n",
        "    h_2 = np.matmul(w_12, h_1) + b_2\n",
        "    h_2 = sigmoid(h_2)\n",
        "\n",
        "    h_3 = np.matmul(w_23, h_2) + b_3\n",
        "    h_3 = sigmoid(h_3)\n",
        "\n",
        "    out = np.matmul(w_3ou, h_3) + b_out\n",
        "    out = softmax(out) #applying a softmax activation\n",
        "\n",
        "    #calculation of the cost\n",
        "    for k in range(num_outputs):\n",
        "      if k == y_train[a]:\n",
        "        cost += 0.5*m.pow(out[k] - 1, 2)\n",
        "      else:\n",
        "        cost += 0.5*m.pow(out[k], 2)\n",
        "\n",
        "    #Back Propogation\n",
        "\n",
        "    #calculation of the derivatives of the cost function with respect to parameters that directly affect it\n",
        "    dC_dw_3ou = np.zeros((10, 16))\n",
        "    dC_db_out = np.zeros((10,))\n",
        "    dC_dh_3 = np.zeros((16,))\n",
        "    for i in range(10):\n",
        "      for j in range(16):\n",
        "        if i == y_train[a]:\n",
        "          dC_dw_3ou[i][j] += (out[i] - 1)*h_3[j]*out[i]*(1 - out[i])\n",
        "          dC_db_out[i] += (out[i] - 1)*out[i]*(1 - out[i])\n",
        "          dC_dh_3[j] += (out[i] - 1)*w_3ou[i][j]*out[i]*(1 - out[i])\n",
        "        else:\n",
        "          dC_dw_3ou[i][j] += out[i]*h_3[j]*out[i]*(1 - out[i])\n",
        "          dC_db_out[i] += out[i]*out[i]*(1 - out[i])\n",
        "          dC_dh_3[j] += out[i]*w_3ou[i][j]*out[i]*(1 - out[i])\n",
        "\n",
        "\n",
        "    #calculation of the effect of the weights and biases on the third hidden layer\n",
        "    dh_3_dw_23 = np.zeros((16, 40))\n",
        "    dh_3_db_3 = np.zeros((16,))\n",
        "    dh_3_dh_2 = np.zeros((16, 40))\n",
        "    for i in range(16):\n",
        "      for j in range(40):\n",
        "        dh_3_dw_23[i][j] += h_2[j]*h_3[i]*(1 - h_3[i])\n",
        "        dh_3_db_3[i] += 1*h_3[i]*(1 - h_3[i])\n",
        "        dh_3_dh_2[i][j] += w_23[i][j]*h_3[i]*(1 - h_3[i])\n",
        "\n",
        "\n",
        "    #calculation of the effect of the weights and biases on the second hidden layer\n",
        "    dh_2_dw_12 = np.zeros((40, 64))\n",
        "    dh_2_db_2 = np.zeros((40,))\n",
        "    dh_2_dh_1 = np.zeros((40, 64))\n",
        "    for i in range(40):\n",
        "      for j in range(64):\n",
        "        dh_2_dw_12[i][j] += h_1[j]*h_2[i]*(1 - h_2[i])\n",
        "        dh_2_db_2[i] += 1*h_2[i]*(1 - h_2[i])\n",
        "        dh_2_dh_1[i][j] += w_12[i][j]*h_2[i]*(1 - h_2[i])\n",
        "\n",
        "\n",
        "    #calculation of the effect of the weights and biases on the first hidden layer\n",
        "    dh_1_dw_in1 = np.zeros((64, 784))\n",
        "    dh_1_db_1 = np.zeros((64,))\n",
        "    dh_1_dinp = np.zeros((64, 784))\n",
        "    for i in range(64):\n",
        "      for j in range(784):\n",
        "        dh_1_dw_in1[i][j] += inp[j]*h_1[i]*(1 - h_1[i])\n",
        "        dh_1_db_1[i] += 1*h_1[i]*(1 - h_1[i])\n",
        "        dh_1_dinp[i][j] += w_in1[i][j]*h_1[i]*(1 - h_1[i])\n",
        "\n",
        "  #dividing through by number of training examples to find the average cost per image\n",
        "  dC_dw_3ou = np.divide(dC_dw_3ou, batch_size)\n",
        "  dC_db_out = np.divide(dC_db_out, batch_size)\n",
        "  dC_dh_3 = np.divide(dC_dh_3, batch_size)\n",
        "  dh_3_dw_23 = np.divide(dh_3_dw_23, batch_size)\n",
        "  dh_3_db_3 = np.divide(dh_3_db_3, batch_size)\n",
        "  dh_3_dh_2 = np.divide(dh_3_dh_2, batch_size)\n",
        "  dh_2_dw_12 = np.divide(dh_2_dw_12, batch_size)\n",
        "  dh_2_db_2 = np.divide(dh_2_db_2, batch_size)\n",
        "  dh_2_dh_1 = np.divide(dh_2_dh_1, batch_size)\n",
        "  dh_1_dw_in1 = np.divide(dh_1_dw_in1, batch_size)\n",
        "  dh_1_db_1 = np.divide(dh_1_db_1, batch_size)\n",
        "  dh_1_dinp = np.divide(dh_1_dinp, batch_size)\n",
        "\n",
        "  #calculating the indirect effect of the first and second hidden layers on the overall cost\n",
        "  dC_dh_1 = dC_dh_3@dh_3_dh_2@dh_2_dh_1\n",
        "  dC_dh_2 = dC_dh_3@dh_3_dh_2\n",
        "\n",
        "  #Updating each of the parameters of the network using gradient descent\n",
        "\n",
        "  #updating weights\n",
        "  w_in1 = w_in1 - l*(dC_dh_1@dh_1_dw_in1)\n",
        "  w_12 = w_12 - l*(dC_dh_2@dh_2_dw_12)\n",
        "  w_23 = w_23 - l*(dC_dh_3@dh_3_dw_23)\n",
        "  w_3ou = w_3ou - l*dC_dw_3ou\n",
        "\n",
        "  #updating biases\n",
        "  b_1 = b_1 - l*(dC_dh_1@dh_1_db_1)\n",
        "  b_2 = b_2 - l*(dC_dh_2@dh_2_db_2)\n",
        "  b_3 = b_3 - l*(dC_dh_3@dh_3_db_3)\n",
        "  b_out = b_out - l*dC_db_out\n",
        "\n",
        "  #finding the average cost over the entire batch\n",
        "  cost = cost/batch_size\n",
        "\n",
        "\n",
        "  print(\"Cost at the\", 10 - num_grad_desc_steps, \"th iteration is:\", cost)\n",
        "\n",
        "\n",
        "  num_grad_desc_steps -= 1\n",
        "  if num_grad_desc_steps == 0: #terminating condition for the while loop\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj_I887aN8yv",
        "outputId": "c3e6a81c-ca0f-40c4-8b1a-e3eac8577929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at the 0 th iteration is: 0.4609292524777017\n",
            "Cost at the 1 th iteration is: 0.4603712018824933\n",
            "Cost at the 2 th iteration is: 0.4598193362392487\n",
            "Cost at the 3 th iteration is: 0.4592738546790284\n",
            "Cost at the 4 th iteration is: 0.4587349581963784\n",
            "Cost at the 5 th iteration is: 0.45820284934378924\n",
            "Cost at the 6 th iteration is: 0.45767773191506295\n",
            "Cost at the 7 th iteration is: 0.4571598106181884\n",
            "Cost at the 8 th iteration is: 0.45664929073839083\n",
            "Cost at the 9 th iteration is: 0.4561463777921036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model\n",
        "\n",
        "Remember we parsed the given MNIST dataset into train and test datasets? We're going to put that test dataset to use now. With our trained weights and biases we expect our network to now behave intelligently and predict the handwritten digits with a good accuracy.\n",
        "\n",
        "We now only feed the data forward, calculate the activations of the neurons at each of the outputs and simply choose the neuron with the maximum activation as the predicted value. If this matches with the given true value, we increase a \"counter\" and finally calculate the accuracy."
      ],
      "metadata": {
        "id": "3CRcSGpaMD8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0 #counts the number of times the network predicts the value correctly\n",
        "num_imgs = 10000\n",
        "for a in range(num_imgs): #there are 10,000 images in the test dataset\n",
        "\n",
        "  inp = [] #entering the input pixel values\n",
        "  for i in range(num_rows):\n",
        "    for j in range(num_rows):\n",
        "      inp.append(x_test[a][i][j]/255) #normalised pixel values between 0 and 1\n",
        "\n",
        "  #forward pass\n",
        "  h_1 = np.matmul(w_in1, inp) + b_1\n",
        "  h_1 = sigmoid(h_1)\n",
        "\n",
        "  h_2 = np.matmul(w_12, h_1) + b_2\n",
        "  h_2 = sigmoid(h_2)\n",
        "\n",
        "  h_3 = np.matmul(w_23, h_2) + b_3\n",
        "  h_3 = sigmoid(h_3)\n",
        "\n",
        "  out = np.matmul(w_3ou, h_3) + b_out\n",
        "  out = softmax(out)\n",
        "\n",
        "  if out.tolist().index(out.max()) == y_test[a]: #checking if the predicted value matches the true value\n",
        "    count += 1\n",
        "\n",
        "print(\"Accuracy of the neural network is:\", count/num_imgs * 100, \"%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RTNdjkiD1vCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff29d491-abdd-4a91-ed17-509ec34aca0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1006\n"
          ]
        }
      ]
    }
  ]
}